{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 4: Summary Questions\n",
    "<a id=part4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "the meaning of receptive filed is the amount of neurons in the orifinal feature map which the current neurons that are being calculated are iterating over.\n",
    "the bigger the receptive filed is, the the 'higher' the level of features that it produces (straight or curved lines vs. flower or plane in the sky).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "kernel size - the amount of elements that is being taking in to account in the weighted sum of the output neuron. the actual elements that are being taking in to the calculations is depended on the stride and dilation.\n",
    "\n",
    "stride - instead of taking adjacent neurons in to the convolution we are taking them with a constant step size. \n",
    "\n",
    "dilation -  a parameter, which determines per neuron, what distance to take between neurons on which we apply the convolution calculation on. this parameter essentially multiplies the receptive field of the convolution by the dilation size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "calculating from the end of the architecture to the beging - the size for the receptive filed in the end is the 13=7*2-1\n",
    "\n",
    "then maxpool will double the receptive filed to 26=13*2\n",
    "\n",
    "after that the convolution op. will result in 55=5+(26-1)*2 pixels.\n",
    "\n",
    "another max pool will result in 110 pixels.\n",
    "\n",
    "and in the end the last convolution op. will result in 112 pixels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "the explanation for this is that by adding the input to the filter result we are learning additive features in addition to the input data.\n",
    "\n",
    "and thus we are learning different filter compared to before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "flase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "Let $P_i$ be a random variable representing thd dropout, and $x_i$ be the layer ontop the dropout is applied.\n",
    "$a$ be such constant such that $\\displaystyle \\mathop{\\mathbb{E}}{[a\\cdot P_i \\cdot x_i]} = \\displaystyle \\mathop{\\mathbb{E}}{[X]}$\n",
    "\n",
    "$\\displaystyle \\mathop{\\mathbb{E}}{[a\\cdot P_i \\cdot x_i]} = a\\cdot \\displaystyle \\mathop{\\mathbb{E}}{[P_i]}\\cdot \\displaystyle \\mathop{\\mathbb{E}}{[x_i]} = a \\cdot (1-p) \\cdot \\displaystyle \\mathop{\\mathbb{E}}{[x_i]}=\\displaystyle \\mathop{\\mathbb{E}}{[x_i]}\\rightarrow a=\\frac{1}{1-p}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "we will prefer not to use l2 loss. the reason is that l2 loss doesnt consider the entyopy of the distrbution in its weighting but rather it uses only the distance in a euclidean setting. which has a strong inluence over the b vector - something that we dont like in classification tasks.\n",
    "\n",
    "we would prefer using loss funcion like cross entropy or neg. log lilelyhood - something that will encode the probability of a correct classification and  punish outliers in a similar way to non outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-18T10:23:34.854297Z",
     "iopub.status.busy": "2022-01-18T10:23:34.853519Z",
     "iopub.status.idle": "2022-01-18T10:23:34.884129Z",
     "shell.execute_reply": "2022-01-18T10:23:34.884654Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H),\n",
    "        nn.Sigmoid(),\n",
    "    ]*N,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "the most likely resone for this plateau is probebly vanishing gradients - the phenomenon where the gradients becomeing very small and as a qunsecuence thr change to the weights is getting less and less segnificent. which causes the training loss to stay the same. this happens when the network is as deep as this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "we think that this will not solve the problem of vanishing gradients. the sigmoid function takes the R space and converts it to the space (0,1) while the tanh takes the same R space and maps it to the space of (-1,1) this will not a segnificant enghouth of a change (we think). a more sagnificant solution is to use ReLu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "    1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "    1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "    1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:** \n",
    "1. False, with a deep enough of a network vanishing gradients could still be happaning\n",
    "2. False. the gradient of the positive part of ReLU of the input, is 1, and therefore it is not linear by the definition of a linear system.\n",
    "3. true, in the case of a negative enough of an output - the activation will countinue to stay zero. and there for the weights will remane in the negetive part - so the neuron would always output 0 (so basecally dead).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "the main difference between thos methods are the ways by which they are iterating over the data.\n",
    "\n",
    "1. in regular gradient descent we are using all of the samples in the training data in each iteration update. so by this approch we first calculating the loss of all of the samples, after that we are avreging the losses and then we updateting the weights. \n",
    "\n",
    "2. min-batch SGD splits the data into mini batches and for each weight update iteration, it uses a single batch.\n",
    "\n",
    "3. in SGD the weights are being updated after each sample - this is what gives the name to the method, the weights are changing du to the loss of each sample without any consideration to the rest of the samples. this helps for a fast convergence. \n",
    "\n",
    "in genral we can say that sgd converges the fastest but is also the noiesest and GD is the slowest but the more smover.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "    1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "    2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "A. as said in the previous question, SGD has a faster convergence rate compered to GD. and also SGD is much less memory intence then GD - we dont need to store the losses for each sample.\n",
    "\n",
    "B. because of the need to store in memory the loss of each sample - for a large enough of a Data set its imposible to do so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "I would expect the number of iterations to decrease. Increasing the batch size will cause each iteration to take more time but the loss will be more accurate since it is based on more samples. Thus, less iterations will be needed to convege to the local minima. It should be noted that since each iteration will take longer, we don't know if the total time will be more/less/same with larger batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "    1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "    1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "    1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "    1. Training  with SGD requires more memory than with GD.\n",
    "    1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "    1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "A. true, as answerd in the privious questions, in SGD we are calculating the gradient for each sample (and update the weights for each grandied).\n",
    "\n",
    "B. false, in SGD we have more variance comppered to GD. the cuase of this is due to the fact that we dont avreg the losses of all of the sapmles before updating the weghits,instade we do an update for each sample. this causes an increse varience (consider to sample with gradiecnce in different directions.).\n",
    "when we avg the entier sample set we get the direction of the local minimum.  \n",
    "\n",
    "C. true, due to the rendomness movment and the varience of this method we can asccape local minium with higher chance then GD.\n",
    "\n",
    "D. false, like answared in the pre. questions, GD will requier more memory and prehaps cant even be trained due to memory needs of GD.\n",
    "\n",
    "E. False. Neither is guaranteed to converge to a global minima. SGD is better at escaping local minima (where GD might get stuck on) but there are no guarantees in life.\n",
    "\n",
    "F. True. When using SGD with momentus, it helps to escape the local minima. While when using SGD without momentum, we rely on variance and randomness to escape that local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "    1. **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "False, as seen in tutorial 5, we saw that in order to properly back propagate through the layer, one must only have the following:\n",
    "let $x$ be the input to the layer, while $y$ is an inner variable of the layer.\n",
    "we require the following:\n",
    "$argmin_{y}{f(x,y)}=g(x,\\hat{y})$ where $\\hat{y}$ is the optimum value for the layer.\n",
    "\n",
    "we saw from the tutorial that in order to back propagate, one must only get $\\nabla_x g(x,\\hat{y})=-\\nabla_{yy}f(x,y)^{-1}\\cdot\\nabla_{yx}f(x,y)$ in order to get that, we can solve the optimization problem directly, or via descent based solution, thus, the sentence is **false**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "    1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "    2. How can each of these problems be caused by increased depth?\n",
    "    3. Provide a numerical example demonstrating each.\n",
    "    4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Vanishing gradients is where, in deep networks, numerical errors become significant and cause the gradients' values of the losses to deminish to 0 (or almost 0). Exploding gradients is the same except that when the numeric errors cause the gradients to become very very large. A good example for the concept is shown in the [Numeric Algorithms when showing a LU decomposition](https://panoptotech.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=c97eba2f-a243-4375-8b26-aa22009d14d8). In either case, the network becomes untrainable since the weights will either update by too much or barely at all.\n",
    "8. For deep networks, during backpropagation, the calculation of the gradient of the loss is done using the chain rule.\n",
    "    - When the gradients near the output layer are small, their multiplication will cause the value to diminish fast, and by the time we finish the multiplication of the gradients we will have a near 0 value. This is the vanishing gradients.\n",
    "    - When the gradients near the output layer are large, the multiplication of the gradients will increase fast (explode) and that is the exploding gradients.\n",
    "9. Let the net be a conv simple net, without activations, of depth k with a width of 1, such that $y=x\\cdot\\prod_{i=0}^{k}{a_i}$ we can see the derivative w.r.t to the first layer will be $x\\cdot\\prod_{i=1}^{k}{a_i}$.\n",
    "\n",
    "if $a_i<1, a_i= \\dfrac{1}{2}$, the gradient will approach 0 when k is sufficiently large.\n",
    "\n",
    "if $a_i>1$, for example $a_i=2$, it will explode instead\n",
    "\n",
    "10. Assuming I encounter one of these, if the accuracy and/or loss changes significantly with each epoch (without converging ofc), it's likely exploding gradients. If the accuracy and/or the loss don't change (or barely change) between epochs, then it's likely vanishing gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$.\n",
    "  \n",
    "  \n",
    "  \n",
    "  **Answer:** \n",
    "  First thing we derive the loss function w.r.t $\\hat{y}$:\n",
    "  \n",
    "   $\\nabla_{\\hat{y}}\\ell(y,\\hat{y})= -\\dfrac{y}{\\hat{y}}+\\dfrac{1-y}{1-\\hat{y}}$ \n",
    "  \n",
    "  Then derive the frobenius norm of the matrices:\n",
    "  \n",
    "  $\\dfrac{\\partial{||W_i||^2_F}}{\\partial{W_i}} = 2W_i$\n",
    "  \n",
    "  Then derive  $\\hat{y}$ w.r.t (with each variable inside):\n",
    "  \n",
    "   $\\nabla_{b_2}{\\hat{y}} = \\mathbb{1}$\n",
    "   \n",
    "   $\\nabla_{b_1}{\\hat{y}} = \\nabla_{b_1}{\\phi{(W_1\\cdot x^{(i)}+b_1)}\\cdot W_2^{T}} $\n",
    "   \n",
    "   $\\nabla_{x^{(i)}}{\\hat{y}} = W_2 \\cdot \\nabla_{x^{(i)}}\\phi{(W_1\\cdot x^{(i)}+b_1)}^T \\cdot W_1 $\n",
    "\n",
    "   $\\nabla_{W_1}{\\hat{y}} = \\nabla_{x^{(i)}}\\phi{(W_1\\cdot x^{(i)}+b_1)} \\cdot W_2 \\cdot x^T $\n",
    "\n",
    "   $\\nabla_{W_2}{\\hat{y}} = \\phi^{T}{(W_1\\cdot x^{(i)}+b_1)} $\n",
    "   \n",
    "   Then we combine all of them together:\n",
    "\n",
    "$\\Rightarrow \\nabla_{b_2}{L_s} = \\nabla_{\\hat{y}}\\ell(y,\\hat{y})\\cdot \\nabla_{b_2}{\\hat{y}} = \\sum_{i=1}^N(-\\dfrac{y^{(i)}}{\\hat{y}^{(i)}}+\\dfrac{1-y^{(i)}}{1-\\hat{y}^{(i)}}) $\n",
    "  \n",
    "  $\\nabla_{b_1}{L_s} = \\nabla_{\\hat{y}}\\ell(y,\\hat{y})\\cdot \\nabla_{b_1}{\\hat{y}} = \\sum_{i=1}^N(-\\dfrac{y^{(i)}}{\\hat{y}^{(i)}}+\\dfrac{1-y^{(i)}}{1-\\hat{y}^{(i)}})\\cdot \\nabla_{b_1}{\\phi{(W_1\\cdot x^{(i)}+b_1)}\\cdot W_2^{T}} $\n",
    "\n",
    "  $\\nabla_{x^{(i)}}{L_s} = \\nabla_{\\hat{y}}\\ell(y,\\hat{y})\\cdot \\nabla_{x^{(i)}}{\\hat{y}} = \\sum_{i=1}^N(-\\dfrac{y^{(i)}}{\\hat{y}^{(i)}}+\\dfrac{1-y^{(i)}}{1-\\hat{y}^{(i)}})\\cdot  W_2 \\cdot \\nabla_{x^{(i)}}\\phi{(W_1\\cdot x^{(i)}+b_1)}^T \\cdot W_1 $\n",
    "    \n",
    "   $\\nabla_{W_1}{L_s} = \\nabla_{\\hat{y}}\\ell(y,\\hat{y})\\cdot \\nabla_{W_1}{\\hat{y}} +\\lambda\\cdot W_1 = \\sum_{i=1}^N(-\\dfrac{y^{(i)}}{\\hat{y}^{(i)}}+\\dfrac{1-y^{(i)}}{1-\\hat{y}^{(i)}})\\cdot \\nabla_{x^{(i)}}\\phi{(W_1\\cdot x^{(i)}+b_1)} \\cdot W_2 \\cdot x^T +\\lambda\\cdot W_1$\n",
    "\n",
    "$\\nabla_{W_2}{L_s} = \\nabla_{\\hat{y}}\\ell(y,\\hat{y})\\cdot \\nabla_{W_2}{\\hat{y}} +\\lambda\\cdot W_2 = \\sum_{i=1}^N(-\\dfrac{y^{(i)}}{\\hat{y}^{(i)}}+\\dfrac{1-y^{(i)}}{1-\\hat{y}^{(i)}})\\cdot\\phi^{T}{(W_1\\cdot x^{(i)}+b_1)} +\\lambda\\cdot W_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given the following code snippet, implement the custom backward function `part4_affine_backward` in `hw4/answers.py` so that it passes the `assert`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "from hw4.answers import part4_affine_backward\n",
    "\n",
    "N, d_in, d_out = 100, 11, 7\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d_in, dtype=dtype)\n",
    "W = torch.rand(d_out, d_in, requires_grad=True, dtype=dtype)\n",
    "b = torch.rand(d_out, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def affine(X, W, b):\n",
    "    return 0.5 * X @ W.T + b\n",
    "\n",
    "class AffineLayerFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        result = affine(X, W, b)\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return part4_affine_backward(ctx, grad_output)\n",
    "\n",
    "l1 = torch.sum(AffineLayerFunction.apply(X, W, b))\n",
    "l1.backward()\n",
    "W_grad1 = W.grad\n",
    "b_grad1 = b.grad\n",
    "\n",
    "l2 = torch.sum(affine(X, W, b))\n",
    "W.grad = b.grad = None\n",
    "l2.backward()\n",
    "W_grad2 = W.grad\n",
    "b_grad2 = b.grad\n",
    "\n",
    "assert torch.allclose(W_grad1, W_grad2)\n",
    "assert torch.allclose(b_grad1, b_grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "    1. Explain this term and why it's used in the context of a language model.\n",
    "    1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?\n",
    "    \n",
    "    \n",
    "A) Embedding of words are basically a presentation of words which contain hidden space where words which contain relative semantic meaning are closer to each other in this space. This embedding is used in the context of language model and what we mostly care about is the semantic meaning and less about the specific word.\n",
    "\n",
    "B) Yes, We can train the model with raw sequences. As a result, the model will have a much worse accuracy since the training is made with specific words. further more, the model will also not be aware of new words and those will not be handled, even if they are close in meaning to other already trained words, (semantic-wise). By using the embedding, we will be able to expand the vocabulary by only adding new words to the embedding part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "    1. What does `Y` contain? why this output shape?\n",
    "    2. How you would implement `nn.Embedding` yourself using only torch tensors. \n",
    "    \n",
    "A) Y is a tensor which contains the mapping of each sample in X to a 42000 sized vector. What we can see is that the first four dims are exactly the same as X and the fifth dim is 42000. (The ith num in that dim is the position in the ith dim, in that latent space).\n",
    "\n",
    "B) I will take a use of a NxMNxM matrix, where NN is the number of words and MM is the dimension of the latent space. In order to check a word's projection, I would multiply a 11-hot vector by the matrix, which will then lead to extraction of a specific row from the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-18T10:23:35.021821Z",
     "iopub.status.busy": "2022-01-18T10:23:35.021337Z",
     "iopub.status.idle": "2022-01-18T10:23:35.191309Z",
     "shell.execute_reply": "2022-01-18T10:23:35.192511Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of $S$: State whether the following sentences are **true or false**, and explain.\n",
    "    1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "    2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length $S$.\n",
    "    3. TBPTT allows the model to learn relations between input that are at most $S$ timesteps apart.\n",
    "    \n",
    "    \n",
    "A) True. TBPTT is using a modified version of the backpropagation algorithm. In a regular BPTT we are calculating the gradient through time (TT) where in the truncated version, we're only using the lastSS iterations in order to calculate the desired gradient.\n",
    "\n",
    "B) False. We must limit the length of the BP and also the length of the forward propageation.\n",
    "\n",
    "C) False. We learned that even though we use TBPTT, we hold memory of precedent events that are given to us by the hidden state, and therefore the model can learn the relations to them.\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "    1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n",
    "  \n",
    "  \n",
    "A) What has been an enormous contribution for weighing the parts of the source sequence into a relevent weight is the supplament of the attention mechanusm between the encoder and the decoder which lets the model to ficus on the sequence's major parts. The encoder will put focus on the important part and thus understand the meaning of the sentence.\n",
    "This focus will help in the decision of what is gold and what is pulp and that will then help with translating. The responsibilty for encoding the information belongs to the hidden state only. \n",
    "\n",
    "B)   We believe that the model will suffer from the idea of the students. By using the attention model and by forwarding the self attention output to the decoder, the model will suffer from the fact that the encoder will have no idea what is the positional information of the translated sentence. In the contrary, the original model is focusing on what is important in the sentence.\n",
    "\n",
    "  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "    1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "    1. Images generated by the model ($z \\to x'$)?\n",
    "    \n",
    "    \n",
    "    \n",
    "VAE without KL-divergence. The KL-divergence is used as sort of a regularization on the encoder to turn the output's distribution to a normal one.\n",
    "\n",
    "A) If we don't use the regularization, the output model will not match the input data, therefore the x'x' will be very similar to the original xx.\n",
    "\n",
    "B) On the contrary, with no use of regulrization, the model will have a significant tendancy to decode z-x'x-x'. x'x' will also be similar to the data that was trained by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "    1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "    2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "    3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight.\n",
    "    \n",
    "    \n",
    "    \n",
    "A) False. The distribution is not N(0,I) N(0,I) but we do map the space so we can use it.\n",
    "\n",
    "B) False. The decoder is not deterministic, it is random with a normal distribution with a mean based on zz.\n",
    "\n",
    "C) True. Since we cannot estimate the evidence in the denominator of the KL-divergence, we minimize its upper bound in hope that it will be tight enough to give sufficient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "    1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "    2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "    3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "    4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "     5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images.\n",
    "     \n",
    "     \n",
    "A) False. In an ideal world, we would like the differentiator to be as accurate as possible, hopefully of 50%. In the way it would not know the difference between the actual photos and the fake ones. Therefore, the originator loss wouldn't be significantly low since it would catch hits from the differentiator.\n",
    "\n",
    "B) False. The models- descriminator and generator, are two different models. Each one of them has its own cons, loss. \n",
    "\n",
    "C) True. That is correct- What we do is sample a single point from the distribution and then decode it back to a picture.\n",
    "\n",
    "D) False. Something that might cause the discriminator to be \"too good\" is to train it for a few periods. It might be better to weaken it and to allow the genertor to get stronger first.\n",
    "\n",
    "E) False. If the discriminator has a 50% accuracy and also the output images are reasonable then the discriminator might not know to differentiate between real pictures and fake ones, that is wht its feedback to the generator is useless, or even harmful, at worst case senario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have implemented a graph convolutional layer based on the following formula, for a graph with $N$ nodes:\n",
    "$$\n",
    "\\mat{Y}=\\varphi\\left( \\sum_{k=1}^{q} \\mat{\\Delta}^k \\mat{X} \\mat{\\alpha}_k + \\vec{b} \\right).\n",
    "$$\n",
    "    1. Assuming $\\mat{X}$ is the input feature matrix of shape $(N, M)$: what does $\\mat{Y}$ contain in it's rows?\n",
    "    1. Unfortunately, due to a bug in your calculation of the Laplacian matrix, you accidentally zeroed the row and column $i=j=5$ (assume more than 5 nodes in the graph).\n",
    "What would be the effect of this bug on the output of your layer, $\\mat{Y}$?\n",
    "\n",
    "\n",
    " A. By the matrices $\\Delta\\in \\mathbb{R}^{NxN}$, therefore, $Y\\in\\mathbb{R}^{NxM}$ we can see that each row of the matrix YY represents a single vertex in the graph, with the columns being features for that item. We are aware that the powers of the Laplacian matrix represent neighbours of the kk'th degree, wyich means that each column, by index, represents a sort of convolution with that degree of neighbour, with αkαk being the weighting scalar.\n",
    "\n",
    "\n",
    "\n",
    "B. This would lead to vanishing everything related to the 5th node, which will in turn cause Y to lose that information as well. That consequence could be catastrophical or inconsequential. Catastrophical if the node has a structural meaning- root of the graph, connecting 2 trees...  and Inconsequential if its a leaf in a tree, which we can then lose. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have discussed the notion of a Receptive Field in the context of a CNN. How would you define a similar concept in the context of a GCN (i.e. a model comprised of multiple graph convolutional layers)?\n",
    "\n",
    "We could define an open field by the input verices that impact the output vertices. We can basically think that the matrix that we create \"produces\" a new graph at each layer, and therefore we can look at each vertex and its matrix, with how many \"input\" vertices influenced that vertex in the end in the GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
